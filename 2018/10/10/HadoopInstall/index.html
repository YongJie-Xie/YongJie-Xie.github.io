
<!DOCTYPE html>
<html lang="zh-cn" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>抱着西瓜砸地球的博客</title>

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Blog,"> 
    <meta name="description" content="说明: Hive 是基于 Hadoop 安装的, 所以必须符合以下条件

拥有已安装 Hadoop 的服务器集群
确保 Hadoop 集群正常运行 (启动 Hive 或 使用 Hadoop 命令时需,"> 
    <meta name="author" content="YongJie-Xie"> 
    <link rel="alternative" href="atom.xml" title="抱着西瓜砸地球的博客" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
</head>

<body class="loading">
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">Hadoop 安装</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>
    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Hadoop 安装</h1>
        <div class="stuff">
            <span>十月 10, 2018</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/CentOS/">CentOS</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Java/">Java</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/VMware/">VMware</a></li></ul>


        </div>
        <div class="content markdown">
            <p>本文搭建了一个由三节点(Master、Worker1、Worker2)构成的Hadoop完全分布式集群, 并通过Hadoop分布式计算的一个示例测试集群的正确性。<br>本文集群三个节点基于三台虚拟机进行搭建, 节点安装的操作系统为Centos 7 (YUM源), Hadoop版本选取为3.1.1。<br>作者也是初次搭建Hadoop集群, 其间遇到了很多问题, 故希望通过该博客让读者避免。</p>
<h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><h3 id="0-配置要求"><a href="#0-配置要求" class="headerlink" title="0. 配置要求"></a>0. 配置要求</h3><h4 id="a-物理机"><a href="#a-物理机" class="headerlink" title="a) 物理机"></a>a) 物理机</h4><ul>
<li>处理器: 无要求</li>
<li>内存: 至少4G (建议8G或以上)</li>
<li>硬盘: 至少20G (建议50G或以上)</li>
<li>网卡: 可以上网即可</li>
</ul>
<h4 id="b-虚拟机"><a href="#b-虚拟机" class="headerlink" title="b) 虚拟机"></a>b) 虚拟机</h4><ul>
<li>数量: 至少3台 (1台Master、2台Worker)</li>
<li>处理器: 至少1个1核 (建议2个1核或以上)</li>
<li>内存: 至少512M (建议1G或以上)</li>
<li>硬盘: 至少10G (建议20G或以上)</li>
<li>网卡: 使用NAT模式即可</li>
</ul>
<h3 id="1-软件工具下载"><a href="#1-软件工具下载" class="headerlink" title="1. 软件工具下载"></a>1. 软件工具下载</h3><ul>
<li><a href="https://my.vmware.com/web/vmware/details?downloadGroup=WKST1413-WIN&amp;productId=686&amp;rPId=25457" target="_blank" rel="noopener">VMware Workstation 14</a></li>
<li><a href="http://mirrors.aliyun.com/centos/7.5.1804/isos/x86_64/CentOS-7-x86_64-Minimal-1804.iso" target="_blank" rel="noopener">CentOS-7-x86_64-Minimal-1804</a></li>
<li><a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">Java SE Development Kit 8u181</a> –&gt; 选择jdk-8u181-linux-x64.tar.gz</li>
<li><a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz" target="_blank" rel="noopener">Apache Hadoop 3.1.1</a></li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=622" target="_blank" rel="noopener">Xshell 6</a> –&gt; 个人免费, License type 选择 Home and school use, 邮箱获取下载链接</li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=623" target="_blank" rel="noopener">Xftp 6</a> –&gt; 同上</li>
</ul>
<h3 id="2-基础环境搭建"><a href="#2-基础环境搭建" class="headerlink" title="2. 基础环境搭建"></a>2. 基础环境搭建</h3><h4 id="a-安装-VMware-Workstation-物理机"><a href="#a-安装-VMware-Workstation-物理机" class="headerlink" title="a) 安装 VMware Workstation (物理机)"></a>a) 安装 VMware Workstation (物理机)</h4><p>过程略</p>
<h4 id="b-安装-Xshell-6-和-Xftp-6-物理机"><a href="#b-安装-Xshell-6-和-Xftp-6-物理机" class="headerlink" title="b) 安装 Xshell 6 和 Xftp 6 (物理机)"></a>b) 安装 Xshell 6 和 Xftp 6 (物理机)</h4><p>过程略</p>
<h4 id="c-VMware-Workstation-安装-CentOS-7-3台"><a href="#c-VMware-Workstation-安装-CentOS-7-3台" class="headerlink" title="c) VMware Workstation 安装 CentOS 7 (3台)"></a>c) VMware Workstation 安装 CentOS 7 <strong><em>(3台)</em></strong></h4><p>参考<a href="https://yongjie-xie.github.io/2018/10/10/CentOSInstall">这里</a></p>
<h4 id="d-CentOS-7-安装-Java-SE-Development-Kit-3台"><a href="#d-CentOS-7-安装-Java-SE-Development-Kit-3台" class="headerlink" title="d) CentOS 7 安装 Java SE Development Kit (3台)"></a>d) CentOS 7 安装 Java SE Development Kit <strong><em>(3台)</em></strong></h4><p>参考<a href="https://yongjie-xie.github.io/2018/10/10/JDKInstall">这里</a></p>
<h2 id="二、配置集群网络"><a href="#二、配置集群网络" class="headerlink" title="二、配置集群网络"></a>二、配置集群网络</h2><h3 id="1-配置静态-ip-地址"><a href="#1-配置静态-ip-地址" class="headerlink" title="1. 配置静态 ip 地址"></a>1. 配置静态 ip 地址</h3><h4 id="a-查看当前IP地址"><a href="#a-查看当前IP地址" class="headerlink" title="a) 查看当前IP地址"></a>a) 查看当前IP地址</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ifconfig</span><br><span class="line">ens32: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.80.163  netmask 255.255.255.0  broadcast 192.168.80.255</span><br><span class="line">        inet6 fe80::47cd:cc1e:2b45:aac6  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:0c:29:20:af:17  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 93  bytes 11412 (11.1 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 82  bytes 9999 (9.7 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure></p>
<h4 id="b-根据DHCP分配的IP地址配置静态IP"><a href="#b-根据DHCP分配的IP地址配置静态IP" class="headerlink" title="b) 根据DHCP分配的IP地址配置静态IP"></a>b) 根据DHCP分配的IP地址配置静态IP</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">nmtui</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1, Edit a connection</span></span><br><span class="line"><span class="comment"># 2, 选中 ens32 网卡, 并按回车进入编辑</span></span><br><span class="line"><span class="comment"># 3, IPv4 CONFIGURATION &lt;Automatic&gt; 修改为IPv4 CONFIGURATION &lt;Manual&gt;, 并选择右侧 &lt;Show&gt;</span></span><br><span class="line"><span class="comment"># 4, 编辑# Addresses、Gateway、DNS, 选择 &lt;OK&gt; 保存, 再选择 &lt;Back&gt; 返回上一级</span></span><br><span class="line"><span class="comment"># 5, Activate a connection</span></span><br><span class="line"><span class="comment"># 6, 选中 ens32 网卡, 并按回车停用, 再按回车启用, 再选择 &lt;Back&gt; 返回上一级</span></span><br><span class="line"><span class="comment"># 7, Quit</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# nmtui</span><br></pre></td></tr></table></figure></p>
<ul>
<li>Master: Addresses 192.168.80.100/24, Gateway 192.168.80.2, DNS 223.5.5.5</li>
<li>Worker1: Addresses 192.168.80.110/24, Gateway 192.168.80.2, DNS 223.5.5.5</li>
<li>Worker2: Addresses 192.168.80.111/24, Gateway 192.168.80.2, DNS 223.5.5.5</li>
</ul>
<p><img src="https://yongjie-xie.github.io/images/00016.png" alt="网卡配置" title="网卡配置"></p>
<h4 id="c-测试网络连通性"><a href="#c-测试网络连通性" class="headerlink" title="c) 测试网络连通性"></a>c) 测试网络连通性</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ping 192.168.80.100 -c 4</span><br><span class="line">ping 192.168.80.110 -c 4</span><br><span class="line">ping 192.168.80.111 -c 4</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ping 192.168.80.100 -c 4</span><br><span class="line">PING 192.168.80.100 (192.168.80.100) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.80.100: icmp_seq=1 ttl=64 time=0.046 ms</span><br><span class="line">64 bytes from 192.168.80.100: icmp_seq=2 ttl=64 time=0.048 ms</span><br><span class="line">64 bytes from 192.168.80.100: icmp_seq=3 ttl=64 time=0.228 ms</span><br><span class="line">64 bytes from 192.168.80.100: icmp_seq=4 ttl=64 time=0.045 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.80.100 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 2999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.045/0.091/0.228/0.079 ms</span><br><span class="line">[root@localhost ~]# ping 192.168.80.110 -c 4</span><br><span class="line">PING 192.168.80.110 (192.168.80.110) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.80.110: icmp_seq=1 ttl=64 time=0.716 ms</span><br><span class="line">64 bytes from 192.168.80.110: icmp_seq=2 ttl=64 time=0.772 ms</span><br><span class="line">64 bytes from 192.168.80.110: icmp_seq=3 ttl=64 time=0.335 ms</span><br><span class="line">64 bytes from 192.168.80.110: icmp_seq=4 ttl=64 time=0.356 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.80.110 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.335/0.544/0.772/0.202 ms</span><br><span class="line">[root@localhost ~]# ping 192.168.80.111 -c 4</span><br><span class="line">PING 192.168.80.111 (192.168.80.111) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.80.111: icmp_seq=1 ttl=64 time=0.748 ms</span><br><span class="line">64 bytes from 192.168.80.111: icmp_seq=2 ttl=64 time=0.282 ms</span><br><span class="line">64 bytes from 192.168.80.111: icmp_seq=3 ttl=64 time=0.612 ms</span><br><span class="line">64 bytes from 192.168.80.111: icmp_seq=4 ttl=64 time=1.21 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.80.111 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3002ms</span><br><span class="line">rtt min/avg/max/mdev = 0.282/0.714/1.216/0.336 ms</span><br></pre></td></tr></table></figure></p>
<h3 id="2-配置域名-hostname"><a href="#2-配置域名-hostname" class="headerlink" title="2. 配置域名 hostname"></a>2. 配置域名 hostname</h3><h4 id="a-Master"><a href="#a-Master" class="headerlink" title="a) Master"></a>a) Master</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> hadoop.master &gt; /etc/hostname</span><br><span class="line">cat /etc/hostname</span><br><span class="line">shutdown -r now</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Master: </span><br><span class="line">[root@localhost ~]# echo hadoop.master &gt; /etc/hostname </span><br><span class="line">[root@localhost ~]# cat /etc/hostname </span><br><span class="line">hadoop.master</span><br><span class="line">[root@localhost ~]# shutdown -r now // 重启后生效</span><br></pre></td></tr></table></figure></p>
<h4 id="b-Worker1"><a href="#b-Worker1" class="headerlink" title="b) Worker1"></a>b) Worker1</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> hadoop.worker1 &gt; /etc/hostname</span><br><span class="line">cat /etc/hostname</span><br><span class="line">shutdown -r now</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# echo hadoop.worker1 &gt; /etc/hostname</span><br><span class="line">[root@localhost ~]# cat /etc/hostname </span><br><span class="line">hadoop.worker1</span><br><span class="line">[root@localhost ~]# shutdown -r now</span><br></pre></td></tr></table></figure></p>
<h4 id="c-Worker2"><a href="#c-Worker2" class="headerlink" title="c) Worker2"></a>c) Worker2</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> hadoop.worker2 &gt; /etc/hostname</span><br><span class="line">cat /etc/hostname</span><br><span class="line">shutdown -r now</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# echo hadoop.worker2 &gt; /etc/hostname </span><br><span class="line">[root@localhost ~]# cat /etc/hostname </span><br><span class="line">hadoop.worker2</span><br><span class="line">[root@localhost ~]# shutdown -r now</span><br></pre></td></tr></table></figure></p>
<h4 id="d-测试-hostname-是否变更"><a href="#d-测试-hostname-是否变更" class="headerlink" title="d) 测试 hostname 是否变更"></a>d) 测试 hostname 是否变更</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# // 观察左侧是否从 [root@localhost ~]# 变为 [root@hadoop ~]#</span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# hostname</span><br><span class="line">hadoop.master // 观察左侧是否从 localhost.localdomain 变为 hadoop.master</span><br></pre></td></tr></table></figure></p>
<h3 id="3-配置-hosts-文件"><a href="#3-配置-hosts-文件" class="headerlink" title="3. 配置 hosts 文件"></a>3. 配置 hosts 文件</h3><h4 id="a-Master、Worker1、Worker2"><a href="#a-Master、Worker1、Worker2" class="headerlink" title="a) Master、Worker1、Worker2"></a>a) Master、Worker1、Worker2</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">192.168.80.100 master hadoop.master</span><br><span class="line">192.168.80.110 worker1 hadoop.worker1</span><br><span class="line">192.168.80.111 worker2 hadoop.worker2</span><br><span class="line">EOF</span><br><span class="line">cat /etc/hosts</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">&gt; 192.168.80.100 master hadoop.master</span><br><span class="line">&gt; 192.168.80.110 worker1 hadoop.worker1</span><br><span class="line">&gt; 192.168.80.111 worker2 hadoop.worker2</span><br><span class="line">&gt; EOF</span><br><span class="line">[root@hadoop ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.80.100 master hadoop.master</span><br><span class="line">192.168.80.110 worker1 hadoop.worker1</span><br><span class="line">192.168.80.111 worker2 hadoop.worker2</span><br></pre></td></tr></table></figure></p>
<h4 id="b-测试-hosts-是否生效"><a href="#b-测试-hosts-是否生效" class="headerlink" title="b) 测试 hosts 是否生效"></a>b) 测试 hosts 是否生效</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ping master -c 4</span><br><span class="line">ping worker1 -c 4</span><br><span class="line">ping worker2 -c 4</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# ping master -c 4</span><br><span class="line">PING master (192.168.80.100) 56(84) bytes of data.</span><br><span class="line">64 bytes from master (192.168.80.100): icmp_seq=1 ttl=64 time=0.057 ms</span><br><span class="line">64 bytes from master (192.168.80.100): icmp_seq=2 ttl=64 time=0.047 ms</span><br><span class="line">64 bytes from master (192.168.80.100): icmp_seq=3 ttl=64 time=0.076 ms</span><br><span class="line">64 bytes from master (192.168.80.100): icmp_seq=4 ttl=64 time=0.193 ms</span><br><span class="line"></span><br><span class="line">--- master ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.047/0.093/0.193/0.058 ms</span><br><span class="line">[root@hadoop ~]# ping worker1 -c 4</span><br><span class="line">PING worker1 (192.168.80.110) 56(84) bytes of data.</span><br><span class="line">64 bytes from worker1 (192.168.80.110): icmp_seq=1 ttl=64 time=0.746 ms</span><br><span class="line">64 bytes from worker1 (192.168.80.110): icmp_seq=2 ttl=64 time=0.753 ms</span><br><span class="line">64 bytes from worker1 (192.168.80.110): icmp_seq=3 ttl=64 time=0.344 ms</span><br><span class="line">64 bytes from worker1 (192.168.80.110): icmp_seq=4 ttl=64 time=0.361 ms</span><br><span class="line"></span><br><span class="line">--- worker1 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3002ms</span><br><span class="line">rtt min/avg/max/mdev = 0.344/0.551/0.753/0.198 ms</span><br><span class="line">[root@hadoop ~]# ping worker2 -c 4</span><br><span class="line">PING worker2 (192.168.80.111) 56(84) bytes of data.</span><br><span class="line">64 bytes from worker2 (192.168.80.111): icmp_seq=1 ttl=64 time=1.52 ms</span><br><span class="line">64 bytes from worker2 (192.168.80.111): icmp_seq=2 ttl=64 time=0.846 ms</span><br><span class="line">64 bytes from worker2 (192.168.80.111): icmp_seq=3 ttl=64 time=0.532 ms</span><br><span class="line">64 bytes from worker2 (192.168.80.111): icmp_seq=4 ttl=64 time=0.307 ms</span><br><span class="line"></span><br><span class="line">--- worker2 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3003ms</span><br><span class="line">rtt min/avg/max/mdev = 0.307/0.802/1.523/0.458 ms</span><br></pre></td></tr></table></figure></p>
<h2 id="三、配置集群SSH免密登录"><a href="#三、配置集群SSH免密登录" class="headerlink" title="三、配置集群SSH免密登录"></a>三、配置集群SSH免密登录</h2><h3 id="1-在-Master、Worker1、Worker2-中配置用户"><a href="#1-在-Master、Worker1、Worker2-中配置用户" class="headerlink" title="1. 在 Master、Worker1、Worker2 中配置用户"></a>1. 在 Master、Worker1、Worker2 中配置用户</h3><h4 id="a-创建用户"><a href="#a-创建用户" class="headerlink" title="a) 创建用户"></a>a) 创建用户</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -g wheel -m hadoop</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# useradd -g wheel -m hadoop // 必须加入 wheel 用户组</span><br></pre></td></tr></table></figure></p>
<h4 id="a-设置密码"><a href="#a-设置密码" class="headerlink" title="a) 设置密码"></a>a) 设置密码</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# passwd hadoop</span><br><span class="line">Changing password for user hadoop.</span><br><span class="line">New password: // 输入新密码, 这里建议使用 hadoop 为密码</span><br><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span><br><span class="line">Retype new password:  // 输入确认密码</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure></p>
<h3 id="2-配置免密登录-Master"><a href="#2-配置免密登录-Master" class="headerlink" title="2. 配置免密登录 (Master)"></a>2. 配置免密登录 (Master)</h3><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong><br>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su hadoop</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# su hadoop // 切换用户</span><br><span class="line">[hadoop@hadoop root]$</span><br></pre></td></tr></table></figure></p>
<p>或者直接使用 hadoop 用户登录</p>
<h4 id="a-生成密钥对"><a href="#a-生成密钥对" class="headerlink" title="a) 生成密钥对"></a>a) 生成密钥对</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop root]$ ssh-keygen -t rsa // 直接全部回车, 使用默认即可</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/hadoop/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:WwARMZsClK+8YPE2DAVwn1EqRneo67QWYeaELQpR0+g hadoop@hadoop.master</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|o+O+ooO+         |</span><br><span class="line">|.oo*o= =         |</span><br><span class="line">| =+o= o .        |</span><br><span class="line">|++E...   .       |</span><br><span class="line">|oB=+    S .      |</span><br><span class="line">|o.B=     o       |</span><br><span class="line">|.+.+.   .        |</span><br><span class="line">|  =              |</span><br><span class="line">| .               |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure></p>
<h4 id="b-部署公钥"><a href="#b-部署公钥" class="headerlink" title="b) 部署公钥"></a>b) 部署公钥</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop root]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<h4 id="c-文件权限设置"><a href="#c-文件权限设置" class="headerlink" title="c) 文件权限设置"></a>c) 文件权限设置</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop root]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<h4 id="d-部署集群免密登录"><a href="#d-部署集群免密登录" class="headerlink" title="d) 部署集群免密登录"></a>d) 部署集群免密登录</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -p -r ~/.ssh hadoop@worker1:~/</span><br><span class="line">scp -p -r ~/.ssh hadoop@worker2:~/</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop root]$ scp -p -r ~/.ssh hadoop@worker1:~/</span><br><span class="line">The authenticity of host &apos;worker1 (192.168.80.110)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:+mGoks2VGS5ArfTJT7bE589r6sN2BzMrrNwdH13Xj54.</span><br><span class="line">ECDSA key fingerprint is MD5:c5:e8:d0:29:54:98:41:9d:68:d6:5b:09:8e:96:f7:96.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;worker1,192.168.80.110&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">hadoop@worker1&apos;s password: </span><br><span class="line">known_hosts                                              100%  367   252.0KB/s   00:00    </span><br><span class="line">id_rsa                                                   100% 1675     1.2MB/s   00:00    </span><br><span class="line">id_rsa.pub                                               100%  402   144.6KB/s   00:00    </span><br><span class="line">authorized_keys                                          100%  402   152.6KB/s   00:00</span><br><span class="line">[hadoop@hadoop root]$ scp -p -r ~/.ssh hadoop@worker2:~/</span><br><span class="line">The authenticity of host &apos;worker2 (192.168.80.111)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:+mGoks2VGS5ArfTJT7bE589r6sN2BzMrrNwdH13Xj54.</span><br><span class="line">ECDSA key fingerprint is MD5:c5:e8:d0:29:54:98:41:9d:68:d6:5b:09:8e:96:f7:96.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;worker2,192.168.80.111&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">hadoop@worker2&apos;s password: </span><br><span class="line">known_hosts                                              100%  551   444.8KB/s   00:00    </span><br><span class="line">id_rsa                                                   100% 1675   941.7KB/s   00:00    </span><br><span class="line">id_rsa.pub                                               100%  402   280.4KB/s   00:00    </span><br><span class="line">authorized_keys                                          100%  402   131.3KB/s   00:00</span><br></pre></td></tr></table></figure></p>
<h3 id="3-测试集群免密登录"><a href="#3-测试集群免密登录" class="headerlink" title="3. 测试集群免密登录"></a>3. 测试集群免密登录</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ssh master</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">ssh worker1</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">ssh worker2</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop root]$ ssh master // 免密登录, 下同</span><br><span class="line">Last login: Wed Nov 21 17:27:29 2018 from worker1</span><br><span class="line">[hadoop@hadoop ~]$ exit</span><br><span class="line">logout</span><br><span class="line">Connection to master closed.</span><br><span class="line">[hadoop@hadoop root]$ ssh worker1</span><br><span class="line">[hadoop@hadoop ~]$ exit</span><br><span class="line">logout</span><br><span class="line">Connection to worker1 closed.</span><br><span class="line">[hadoop@hadoop root]$ ssh worker2</span><br><span class="line">[hadoop@hadoop ~]$ exit</span><br><span class="line">logout</span><br><span class="line">Connection to worker2 closed.</span><br></pre></td></tr></table></figure></p>
<p>如果不能免密登录, 请检查文件以及文件夹的权限是否如下所示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop .ssh]$ pwd</span><br><span class="line">/home/hadoop/.ssh // 当前路径</span><br><span class="line">[hadoop@hadoop .ssh]$ ls -al</span><br><span class="line">total 16</span><br><span class="line">drwx------. 2 hadoop wheel   80 Nov 21 17:19 . // .ssh 目录权限是 700</span><br><span class="line">drwx------. 3 hadoop wheel   95 Nov 21 17:18 ..</span><br><span class="line">-rw-------. 1 hadoop wheel  402 Nov 21 17:19 authorized_keys // 文件权限是 600</span><br><span class="line">-rw-------. 1 hadoop wheel 1675 Nov 21 17:18 id_rsa</span><br><span class="line">-rw-r--r--. 1 hadoop wheel  402 Nov 21 17:18 id_rsa.pub // 文件权限是644</span><br><span class="line">-rw-r--r--. 1 hadoop wheel  551 Nov 21 17:29 known_hosts</span><br></pre></td></tr></table></figure></p>
<h2 id="四、配置集群防火墙"><a href="#四、配置集群防火墙" class="headerlink" title="四、配置集群防火墙"></a>四、配置集群防火墙</h2><p><strong><em>注意: 必须切换到 root 用户下执行, 三台虚拟机都必须关闭防火墙和SeLinux</em></strong></p>
<h3 id="1-关闭Firewalld"><a href="#1-关闭Firewalld" class="headerlink" title="1. 关闭Firewalld"></a>1. 关闭Firewalld</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop ~]# systemctl disable firewalld</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br></pre></td></tr></table></figure></p>
<h3 id="2-关闭SeLinux"><a href="#2-关闭SeLinux" class="headerlink" title="2. 关闭SeLinux"></a>2. 关闭SeLinux</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">"s#SELINUX=enforcing#SELINUX=disabled#g"</span> /etc/selinux/config</span><br><span class="line">cat /etc/selinux/config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# sed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/config</span><br><span class="line">[root@hadoop ~]# cat /etc/selinux/config </span><br><span class="line"></span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#     enforcing - SELinux security policy is enforced.</span><br><span class="line">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#     disabled - No SELinux policy is loaded.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= can take one of three two values:</span><br><span class="line">#     targeted - Targeted processes are protected,</span><br><span class="line">#     minimum - Modification of targeted policy. Only selected processes are protected. </span><br><span class="line">#     mls - Multi Level Security protection.</span><br><span class="line">SELINUXTYPE=targeted </span><br><span class="line"></span><br><span class="line">[root@hadoop ~]# setenforce 0 // 临时关闭</span><br><span class="line">[root@hadoop ~]# shutdown -r now // 重启后生效, 如果执行了临时关闭, 则可不重启</span><br></pre></td></tr></table></figure></p>
<h2 id="五、安装Hadoop"><a href="#五、安装Hadoop" class="headerlink" title="五、安装Hadoop"></a>五、安装Hadoop</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-Hadoop-安装环境"><a href="#1-Hadoop-安装环境" class="headerlink" title="1. Hadoop 安装环境"></a>1. Hadoop 安装环境</h3><p>3台虚拟机都必须配置 Java 运行环境, 因为 Hadoop 是基于 Java 的。</p>
<h4 id="a-测试-Java-运行环境"><a href="#a-测试-Java-运行环境" class="headerlink" title="a) 测试 Java 运行环境"></a>a) 测试 Java 运行环境</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ java -version</span><br><span class="line">java version &quot;1.8.0_181&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_181-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Hadoop-安装文件下载和解压"><a href="#2-Hadoop-安装文件下载和解压" class="headerlink" title="2. Hadoop 安装文件下载和解压"></a>2. Hadoop 安装文件下载和解压</h3><p><strong><em>注意: Hadoop 放在 home 目录下, 也就是 /home/hadoop/ 目录</em></strong><br>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line">wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz</span><br><span class="line">tar zxf hadoop-3.1.1.tar.gz</span><br><span class="line">mv hadoop-3.1.1 hadoop</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz</span><br><span class="line">--2018-11-21 18:23:46--  http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz</span><br><span class="line">Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.8.193, 2402:f000:1:408:8100::1</span><br><span class="line">Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.8.193|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 334559382 (319M) [application/x-gzip]</span><br><span class="line">Saving to: ‘hadoop-3.1.1.tar.gz’</span><br><span class="line"></span><br><span class="line">100%[=============================================================================&gt;] 334,559,382 4.81MB/s   in 63s    </span><br><span class="line"></span><br><span class="line">2018-11-21 18:25:00 (5.06 MB/s) - ‘hadoop-3.1.1.tar.gz’ saved [334559382/334559382]</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop ~]$ tar zxf hadoop-3.1.1.tar.gz</span><br><span class="line">[hadoop@hadoop ~]$ mv hadoop-3.1.1 hadoop</span><br></pre></td></tr></table></figure></p>
<h2 id="六、配置Hadoop"><a href="#六、配置Hadoop" class="headerlink" title="六、配置Hadoop"></a>六、配置Hadoop</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-core-site-xml"><a href="#1-core-site-xml" class="headerlink" title="1. core-site.xml"></a>1. core-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/hadoop/etc/hadoop/core-site.xml</span><br><span class="line">cat ~/hadoop/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/hadoop/etc/hadoop/core-site.xml </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/core-site.xml </span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 指定HDFS的NameService --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 临时路径 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/home/hadoop/hadoop/tmp&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-hdfs-site-xml"><a href="#2-hdfs-site-xml" class="headerlink" title="2. hdfs-site.xml"></a>2. hdfs-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/hadoop/etc/hadoop/hdfs-site.xml</span><br><span class="line">cat ~/hadoop/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/hadoop/etc/hadoop/hdfs-site.xml </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/hdfs-site.xml </span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 配置DataNode --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 配置NameNode --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="3-mapred-site-xml"><a href="#3-mapred-site-xml" class="headerlink" title="3. mapred-site.xml"></a>3. mapred-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/hadoop/etc/hadoop/mapred-site.xml</span><br><span class="line">cat ~/hadoop/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/hadoop/etc/hadoop/mapred-site.xml </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/mapred-site.xml </span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 定义计算框架 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=/home/hadoop/hadoop&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=/home/hadoop/hadoop&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=/home/hadoop/hadoop&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="4-yarn-site-xml"><a href="#4-yarn-site-xml" class="headerlink" title="4. yarn-site.xml"></a>4. yarn-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">cat ~/hadoop/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/hadoop/etc/hadoop/yarn-site.xml </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/yarn-site.xml </span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 配置ResourceManager --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 配置监听地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.0.0.0:8088&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 配置NodeManager --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!-- 配置内存大小 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!-- 配置物理内存和虚拟内存比率 --&gt;</span><br><span class="line">  &lt;property&gt;  </span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;  </span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 配置历史服务器 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 配置内存极限值 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 配置CPU核心极限值 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="5-workers"><a href="#5-workers" class="headerlink" title="5. workers"></a>5. workers</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/hadoop/etc/hadoop/workers</span><br><span class="line">cat ~/hadoop/etc/hadoop/workers</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/hadoop/etc/hadoop/workers </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/workers </span><br><span class="line">worker1</span><br><span class="line">worker2</span><br></pre></td></tr></table></figure></p>
<h3 id="6-hadoop-env-sh"><a href="#6-hadoop-env-sh" class="headerlink" title="6. hadoop-env.sh"></a>6. hadoop-env.sh</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">"s@# export JAVA_HOME=@export JAVA_HOME=<span class="variable">$JAVA_HOME</span>@g"</span> ~/hadoop/etc/hadoop/hadoop-env.sh</span><br><span class="line">cat ~/hadoop/etc/hadoop/hadoop-env.sh | grep JAVA_HOME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前请确保环境变量设置正确</span></span><br><span class="line"><span class="comment"># $JAVA_HOME</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ sed -i &quot;s@# export JAVA_HOME=@export JAVA_HOME=$JAVA_HOME@g&quot; ~/hadoop/etc/hadoop/hadoop-env.sh</span><br><span class="line">[hadoop@hadoop ~]$ cat ~/hadoop/etc/hadoop/hadoop-env.sh | grep JAVA_HOME</span><br><span class="line">#  JAVA_HOME=/usr/java/testing hdfs dfs -ls</span><br><span class="line"># Technically, the only required environment variable is JAVA_HOME.</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_181 // 增加</span><br></pre></td></tr></table></figure></p>
<h3 id="7-环境变量-bashrc"><a href="#7-环境变量-bashrc" class="headerlink" title="7. 环境变量 .bashrc"></a>7. 环境变量 .bashrc</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">cat ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/.bashrc </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/.bashrc </span><br><span class="line"># .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line">if [ -f /etc/bashrc ]; then</span><br><span class="line">    . /etc/bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Uncomment the following line if you don&apos;t like systemctl&apos;s auto-paging feature:</span><br><span class="line"># export SYSTEMD_PAGER=</span><br><span class="line"></span><br><span class="line"># User specific aliases and functions</span><br><span class="line"></span><br><span class="line"># Hadoop Env</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br><span class="line">[hadoop@hadoop ~]$ source ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<h3 id="8-部署集群-Hadoop"><a href="#8-部署集群-Hadoop" class="headerlink" title="8. 部署集群 Hadoop"></a>8. 部署集群 Hadoop</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.bashrc hadoop@worker1:~/</span><br><span class="line">scp -r ~/hadoop hadoop@worker1:~/</span><br><span class="line">scp ~/.bashrc hadoop@worker2:~/</span><br><span class="line">scp -r ~/hadoop hadoop@worker2:~/</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ scp ~/.bashrc hadoop@worker1:~/</span><br><span class="line">.bashrc                                                  100%  577   306.2KB/s   00:00</span><br><span class="line">[hadoop@hadoop ~]$ scp -r ~/hadoop hadoop@worker1:~/</span><br><span class="line">...</span><br><span class="line">maven-base.css                                           100% 2421   953.6KB/s   00:00    </span><br><span class="line">site.css                                                 100%   53    38.4KB/s   00:00    </span><br><span class="line">maven-theme.css                                          100% 4624     1.6MB/s   00:00    </span><br><span class="line">print.css                                                100%  215    16.2KB/s   00:00    </span><br><span class="line">dependency-analysis.html                                 100%   25KB   7.3MB/s   00:00    </span><br><span class="line">[hadoop@hadoop ~]$ scp ~/.bashrc hadoop@worker2:~/</span><br><span class="line">.bashrc                                                  100%  577   562.0KB/s   00:00</span><br><span class="line">[hadoop@hadoop ~]$ scp -r ~/hadoop hadoop@worker2:~/</span><br><span class="line">...</span><br><span class="line">maven-base.css                                           100% 2421   855.1KB/s   00:00    </span><br><span class="line">site.css                                                 100%   53    23.5KB/s   00:00    </span><br><span class="line">maven-theme.css                                          100% 4624     1.2MB/s   00:00    </span><br><span class="line">print.css                                                100%  215    19.5KB/s   00:00    </span><br><span class="line">dependency-analysis.html                                 100%   25KB   9.1MB/s   00:00</span><br></pre></td></tr></table></figure></p>
<h2 id="七、启动Hadoop"><a href="#七、启动Hadoop" class="headerlink" title="七、启动Hadoop"></a>七、启动Hadoop</h2><h3 id="1-启动-Hadoop-集群"><a href="#1-启动-Hadoop-集群" class="headerlink" title="1. 启动 Hadoop 集群"></a>1. 启动 Hadoop 集群</h3><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong><br>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br><span class="line">start-all.sh</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ hadoop namenode -format</span><br><span class="line">WARNING: Use of this script to execute namenode is deprecated.</span><br><span class="line">WARNING: Attempting to execute replacement &quot;hdfs namenode&quot; instead.</span><br><span class="line"></span><br><span class="line">2018-11-22 14:07:19,620 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = master/192.168.80.100</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 3.1.1</span><br><span class="line">STARTUP_MSG:   classpath = /home/hadoop/hadoop/etc/hadoop: ... :/home/hadoop/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar</span><br><span class="line">STARTUP_MSG:   build = https://github.com/apache/hadoop -r 2b9a8c1d3a2caf1e733d57f346af3ff0d5ba529c; compiled by &apos;leftnoteasy&apos; on 2018-08-02T04:26Z</span><br><span class="line">STARTUP_MSG:   java = 1.8.0_181</span><br><span class="line">************************************************************/</span><br><span class="line">2018-11-22 14:07:19,697 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">2018-11-22 14:07:19,730 INFO namenode.NameNode: createNameNode [-format]</span><br><span class="line">2018-11-22 14:07:20,746 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Formatting using clusterid: CID-53eb707f-2c13-43ba-ab87-a9d02c67931c</span><br><span class="line">2018-11-22 14:07:23,591 INFO namenode.FSEditLog: Edit logging is async:true</span><br><span class="line">2018-11-22 14:07:23,674 INFO namenode.FSNamesystem: KeyProvider: null</span><br><span class="line">2018-11-22 14:07:23,678 INFO namenode.FSNamesystem: fsLock is fair: true</span><br><span class="line">2018-11-22 14:07:23,709 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false</span><br><span class="line">2018-11-22 14:07:23,768 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)</span><br><span class="line">2018-11-22 14:07:23,768 INFO namenode.FSNamesystem: supergroup          = supergroup</span><br><span class="line">2018-11-22 14:07:23,768 INFO namenode.FSNamesystem: isPermissionEnabled = true</span><br><span class="line">2018-11-22 14:07:23,769 INFO namenode.FSNamesystem: HA Enabled: false</span><br><span class="line">2018-11-22 14:07:23,921 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling</span><br><span class="line">2018-11-22 14:07:24,005 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000</span><br><span class="line">2018-11-22 14:07:24,005 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true</span><br><span class="line">2018-11-22 14:07:24,048 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000</span><br><span class="line">2018-11-22 14:07:24,049 INFO blockmanagement.BlockManager: The block deletion will start around 2018 Nov 22 14:07:24</span><br><span class="line">2018-11-22 14:07:24,057 INFO util.GSet: Computing capacity for map BlocksMap</span><br><span class="line">2018-11-22 14:07:24,057 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">2018-11-22 14:07:24,077 INFO util.GSet: 2.0% max memory 235.9 MB = 4.7 MB</span><br><span class="line">2018-11-22 14:07:24,077 INFO util.GSet: capacity      = 2^19 = 524288 entries</span><br><span class="line">2018-11-22 14:07:24,123 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false</span><br><span class="line">2018-11-22 14:07:24,140 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS</span><br><span class="line">2018-11-22 14:07:24,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033</span><br><span class="line">2018-11-22 14:07:24,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0</span><br><span class="line">2018-11-22 14:07:24,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000</span><br><span class="line">2018-11-22 14:07:24,140 INFO blockmanagement.BlockManager: defaultReplication         = 2</span><br><span class="line">2018-11-22 14:07:24,141 INFO blockmanagement.BlockManager: maxReplication             = 512</span><br><span class="line">2018-11-22 14:07:24,141 INFO blockmanagement.BlockManager: minReplication             = 1</span><br><span class="line">2018-11-22 14:07:24,141 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2</span><br><span class="line">2018-11-22 14:07:24,141 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms</span><br><span class="line">2018-11-22 14:07:24,153 INFO blockmanagement.BlockManager: encryptDataTransfer        = false</span><br><span class="line">2018-11-22 14:07:24,154 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000</span><br><span class="line">2018-11-22 14:07:24,524 INFO util.GSet: Computing capacity for map INodeMap</span><br><span class="line">2018-11-22 14:07:24,524 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">2018-11-22 14:07:24,525 INFO util.GSet: 1.0% max memory 235.9 MB = 2.4 MB</span><br><span class="line">2018-11-22 14:07:24,525 INFO util.GSet: capacity      = 2^18 = 262144 entries</span><br><span class="line">2018-11-22 14:07:24,526 INFO namenode.FSDirectory: ACLs enabled? false</span><br><span class="line">2018-11-22 14:07:24,526 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true</span><br><span class="line">2018-11-22 14:07:24,526 INFO namenode.FSDirectory: XAttrs enabled? true</span><br><span class="line">2018-11-22 14:07:24,526 INFO namenode.NameNode: Caching file names occurring more than 10 times</span><br><span class="line">2018-11-22 14:07:24,548 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, ... , maxSnapshotLimit: 65536</span><br><span class="line">2018-11-22 14:07:24,580 INFO snapshot.SnapshotManager: SkipList is disabled</span><br><span class="line">2018-11-22 14:07:24,622 INFO util.GSet: Computing capacity for map cachedBlocks</span><br><span class="line">2018-11-22 14:07:24,622 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">2018-11-22 14:07:24,622 INFO util.GSet: 0.25% max memory 235.9 MB = 603.8 KB</span><br><span class="line">2018-11-22 14:07:24,622 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2018-11-22 14:07:24,696 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10</span><br><span class="line">2018-11-22 14:07:24,696 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10</span><br><span class="line">2018-11-22 14:07:24,696 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25</span><br><span class="line">2018-11-22 14:07:24,702 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">2018-11-22 14:07:24,703 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">2018-11-22 14:07:24,756 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">2018-11-22 14:07:24,757 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">2018-11-22 14:07:24,757 INFO util.GSet: 0.029999999329447746% max memory 235.9 MB = 72.5 KB</span><br><span class="line">2018-11-22 14:07:24,757 INFO util.GSet: capacity      = 2^13 = 8192 entries</span><br><span class="line">2018-11-22 14:07:24,843 INFO namenode.FSImage: Allocated new BlockPoolId: BP-125441630-192.168.80.100-1542866844822</span><br><span class="line">2018-11-22 14:07:24,905 INFO common.Storage: Storage directory /home/hadoop/hadoop/tmp/dfs/name has been successfully formatted.</span><br><span class="line">2018-11-22 14:07:24,962 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2018-11-22 14:07:25,268 INFO namenode.FSImageFormatProtobuf: Image file /home/ ... /fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds .</span><br><span class="line">2018-11-22 14:07:25,380 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2018-11-22 14:07:25,431 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.80.100</span><br><span class="line">************************************************************/</span><br><span class="line">[hadoop@hadoop ~]$ start-all.sh </span><br><span class="line">WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.</span><br><span class="line">WARNING: This is not a recommended production deployment configuration.</span><br><span class="line">WARNING: Use CTRL-C to abort.</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">Starting datanodes</span><br><span class="line">worker2: WARNING: /home/hadoop/hadoop/logs does not exist. Creating.</span><br><span class="line">worker1: WARNING: /home/hadoop/hadoop/logs does not exist. Creating.</span><br><span class="line">Starting secondary namenodes [hadoop.master]</span><br><span class="line">hadoop.master: Warning: Permanently added &apos;hadoop.master&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">2018-11-22 14:44:58,370 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br></pre></td></tr></table></figure></p>
<h3 id="2-查看-Hadoop-集群启动状态"><a href="#2-查看-Hadoop-集群启动状态" class="headerlink" title="2. 查看 Hadoop 集群启动状态"></a>2. 查看 Hadoop 集群启动状态</h3><h4 id="a-Master-1"><a href="#a-Master-1" class="headerlink" title="a) Master"></a>a) Master</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ jps</span><br><span class="line">2833 Jps</span><br><span class="line">2085 NameNode</span><br><span class="line">2267 SecondaryNameNode</span><br><span class="line">2511 ResourceManager</span><br></pre></td></tr></table></figure></p>
<h4 id="b-Worker1-1"><a href="#b-Worker1-1" class="headerlink" title="b) Worker1"></a>b) Worker1</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ jps</span><br><span class="line">1627 NodeManager</span><br><span class="line">1517 DataNode</span><br><span class="line">1742 Jps</span><br></pre></td></tr></table></figure></p>
<h4 id="c-Worker2-1"><a href="#c-Worker2-1" class="headerlink" title="c) Worker2"></a>c) Worker2</h4><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ jps</span><br><span class="line">1627 NodeManager</span><br><span class="line">1517 DataNode</span><br><span class="line">1742 Jps</span><br></pre></td></tr></table></figure></p>
<p>Hadoop 集群到此安装完毕, 下面进行集群测试</p>
<h2 id="八、测试Hadoop"><a href="#八、测试Hadoop" class="headerlink" title="八、测试Hadoop"></a>八、测试Hadoop</h2><p><strong><em>注意: 必须切换到 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-查看网页"><a href="#1-查看网页" class="headerlink" title="1. 查看网页"></a>1. 查看网页</h3><p>建议修改物理机 hosts 文件, 自行百度教程<br>Windows: C:\Windows\System32\drivers\etc\hosts<br>Linux: /etc/hosts<br>网址: <a href="http://master:9870" target="_blank" rel="noopener">http://master:9870</a> 或 <a href="http://192.168.80.100:9870" target="_blank" rel="noopener">http://192.168.80.100:9870</a><br><img src="https://yongjie-xie.github.io/images/00017.png" alt="http://master:9870" title="http://master:9870"><br>网址: <a href="http://master:8088" target="_blank" rel="noopener">http://master:8088</a> 或 <a href="http://192.168.80.100:8088" target="_blank" rel="noopener">http://192.168.80.100:8088</a><br><img src="https://yongjie-xie.github.io/images/00018.png" alt="http://master:8088" title="http://master:8088"></p>
<h3 id="2-测试-Hadoop-自带的-WordCount-实例"><a href="#2-测试-Hadoop-自带的-WordCount-实例" class="headerlink" title="2. 测试 Hadoop 自带的 WordCount 实例"></a>2. 测试 Hadoop 自带的 WordCount 实例</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br><span class="line">hadoop fs -put ~/hadoop/LICENSE.txt /input/</span><br><span class="line">hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar wordcount /input /output</span><br><span class="line">hadoop fs -cat /output/part-r-00000</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ hadoop fs -mkdir /input</span><br><span class="line">2018-11-22 15:45:54,366 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop ~]$ hadoop fs -put ~/hadoop/LICENSE.txt /input/</span><br><span class="line">2018-11-22 16:02:53,020 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop ~]$ hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar wordcount /input /output</span><br><span class="line">2018-11-22 16:49:37,533 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">2018-11-22 16:49:39,787 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.80.100:8032</span><br><span class="line">2018-11-22 16:49:42,021 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1542876520119_0001</span><br><span class="line">2018-11-22 16:49:43,364 INFO input.FileInputFormat: Total input files to process : 1</span><br><span class="line">2018-11-22 16:49:43,561 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">2018-11-22 16:49:43,626 INFO Configuration.deprecation: y.r.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">2018-11-22 16:49:44,004 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1542876520119_0001</span><br><span class="line">2018-11-22 16:49:44,005 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2018-11-22 16:49:44,461 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2018-11-22 16:49:44,475 INFO resource.ResourceUtils: Unable to find &apos;resource-types.xml&apos;.</span><br><span class="line">2018-11-22 16:49:45,269 INFO impl.YarnClientImpl: Submitted application application_1542876520119_0001</span><br><span class="line">2018-11-22 16:49:45,345 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1542876520119_0001/</span><br><span class="line">2018-11-22 16:49:45,346 INFO mapreduce.Job: Running job: job_1542876520119_0001</span><br><span class="line">2018-11-22 16:50:06,628 INFO mapreduce.Job: Job job_1542876520119_0001 running in uber mode : false</span><br><span class="line">2018-11-22 16:50:06,629 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2018-11-22 16:50:22,993 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2018-11-22 16:50:36,194 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2018-11-22 16:50:36,230 INFO mapreduce.Job: Job job_1542876520119_0001 completed successfully</span><br><span class="line">2018-11-22 16:50:36,458 INFO mapreduce.Job: Counters: 53</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=46271</span><br><span class="line">        FILE: Number of bytes written=521587</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=147245</span><br><span class="line">        HDFS: Number of bytes written=34795</span><br><span class="line">        HDFS: Number of read operations=8</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Launched reduce tasks=1</span><br><span class="line">        Data-local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=25352</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=21136</span><br><span class="line">        Total time spent by all map tasks (ms)=12676</span><br><span class="line">        Total time spent by all reduce tasks (ms)=10568</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=12676</span><br><span class="line">        Total vcore-milliseconds taken by all reduce tasks=10568</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=12980224</span><br><span class="line">        Total megabyte-milliseconds taken by all reduce tasks=10821632</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=2746</span><br><span class="line">        Map output records=21463</span><br><span class="line">        Map output bytes=228869</span><br><span class="line">        Map output materialized bytes=46271</span><br><span class="line">        Input split bytes=101</span><br><span class="line">        Combine input records=21463</span><br><span class="line">        Combine output records=2965</span><br><span class="line">        Reduce input groups=2965</span><br><span class="line">        Reduce shuffle bytes=46271</span><br><span class="line">        Reduce input records=2965</span><br><span class="line">        Reduce output records=2965</span><br><span class="line">        Spilled Records=5930</span><br><span class="line">        Shuffled Maps =1</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=1</span><br><span class="line">        GC time elapsed (ms)=479</span><br><span class="line">        CPU time spent (ms)=3350</span><br><span class="line">        Physical memory (bytes) snapshot=320204800</span><br><span class="line">        Virtual memory (bytes) snapshot=5475635200</span><br><span class="line">        Total committed heap usage (bytes)=142741504</span><br><span class="line">        Peak Map Physical memory (bytes)=205881344</span><br><span class="line">        Peak Map Virtual memory (bytes)=2733322240</span><br><span class="line">        Peak Reduce Physical memory (bytes)=114323456</span><br><span class="line">        Peak Reduce Virtual memory (bytes)=2742312960</span><br><span class="line">    Shuffle Errors</span><br><span class="line">        BAD_ID=0</span><br><span class="line">        CONNECTION=0</span><br><span class="line">        IO_ERROR=0</span><br><span class="line">        WRONG_LENGTH=0</span><br><span class="line">        WRONG_MAP=0</span><br><span class="line">        WRONG_REDUCE=0</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=147144</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=34795</span><br><span class="line">[hadoop@hadoop ~]$ hadoop fs -cat /output/part-r-00000</span><br><span class="line">2018-11-22 16:54:17,392 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">...</span><br><span class="line">“Your”) 2</span><br><span class="line">“You”   4</span><br><span class="line">“as 1</span><br><span class="line">“commercial 3</span><br><span class="line">“control”   2</span><br></pre></td></tr></table></figure></p>
<p><strong><em>Hadoop 集群安装配置结束</em></strong></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title='0' data-url='http://link.hhtjim.com/163/35032211.mp3'></li>
                    
                        <li title='1' data-url='http://link.hhtjim.com/163/41500546.mp3'></li>
                    
                        <li title='2' data-url='http://link.hhtjim.com/163/555557845.mp3'></li>
                    
                        <li title='3' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='4' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='5' data-url='http://link.hhtjim.com/163/409650440.mp3'></li>
                    
                        <li title='6' data-url='http://link.hhtjim.com/163/409654855.mp3'></li>
                    
                        <li title='7' data-url='http://link.hhtjim.com/163/407761545.mp3'></li>
                    
                        <li title='8' data-url='http://link.hhtjim.com/163/409654856.mp3'></li>
                    
                        <li title='9' data-url='http://link.hhtjim.com/163/466327445.mp3'></li>
                    
                        <li title='10' data-url='http://link.hhtjim.com/163/409654857.mp3'></li>
                    
                        <li title='11' data-url='http://link.hhtjim.com/163/405987145.mp3'></li>
                    
                        <li title='12' data-url='http://link.hhtjim.com/163/474945732.mp3'></li>
                    
                        <li title='13' data-url='http://link.hhtjim.com/163/409654858.mp3'></li>
                    
                        <li title='14' data-url='http://link.hhtjim.com/163/452578261.mp3'></li>
                    
                        <li title='15' data-url='http://link.hhtjim.com/163/409654859.mp3'></li>
                    
                        <li title='16' data-url='http://link.hhtjim.com/163/435307781.mp3'></li>
                    
                        <li title='17' data-url='http://link.hhtjim.com/163/477145631.mp3'></li>
                    
                        <li title='18' data-url='http://link.hhtjim.com/163/456175343.mp3'></li>
                    
                        <li title='19' data-url='http://link.hhtjim.com/163/409654860.mp3'></li>
                    
                        <li title='20' data-url='http://link.hhtjim.com/163/408250066.mp3'></li>
                    
                        <li title='21' data-url='http://link.hhtjim.com/163/537262197.mp3'></li>
                    
                        <li title='22' data-url='http://link.hhtjim.com/163/447978200.mp3'></li>
                    
                        <li title='23' data-url='http://link.hhtjim.com/163/439625573.mp3'></li>
                    
                        <li title='24' data-url='http://link.hhtjim.com/163/457840567.mp3'></li>
                    
                        <li title='25' data-url='http://link.hhtjim.com/163/452575951.mp3'></li>
                    
                        <li title='26' data-url='http://link.hhtjim.com/163/454966416.mp3'></li>
                    
                        <li title='27' data-url='http://link.hhtjim.com/163/419375942.mp3'></li>
                    
                        <li title='28' data-url='http://link.hhtjim.com/163/411988718.mp3'></li>
                    
                        <li title='29' data-url='http://link.hhtjim.com/163/418279656.mp3'></li>
                    
                        <li title='30' data-url='http://link.hhtjim.com/163/417594827.mp3'></li>
                    
                        <li title='31' data-url='http://link.hhtjim.com/163/423104116.mp3'></li>
                    
                        <li title='32' data-url='http://link.hhtjim.com/163/452577615.mp3'></li>
                    
                        <li title='33' data-url='http://link.hhtjim.com/163/436675327.mp3'></li>
                    
                        <li title='34' data-url='http://link.hhtjim.com/163/413074476.mp3'></li>
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
        data-ae='false'
        data-ci='6f2ced5993cec2abfdc0'
        data-cs='53941abd6c61cb31c982bddf0394826699a4b9a7'
        data-r='YongJie-Xie.github.io'
        data-o='YongJie-Xie'
        data-a='YongJie-Xie'
        data-d='false'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>