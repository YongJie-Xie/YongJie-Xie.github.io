
<!DOCTYPE html>
<html lang="zh-cn" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>抱着西瓜砸地球的博客</title>

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Blog,"> 
    <meta name="description" content="Hive 是基于 Hadoop 的一个数据仓库工具, 可以将结构化的数据文件映射为一张数据库表, 并提供简单的sql查询功能, 可以将sql语句转换为MapReduce任务进行运行
说明: Hive,"> 
    <meta name="author" content="YongJie-Xie"> 
    <link rel="alternative" href="atom.xml" title="抱着西瓜砸地球的博客" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
</head>

<body class="loading">
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">Hive 安装</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>
    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Hive 安装</h1>
        <div class="stuff">
            <span>十二月 06, 2018</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/CentOS/">CentOS</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hive/">Hive</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Java/">Java</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/VMware/">VMware</a></li></ul>


        </div>
        <div class="content markdown">
            <p>Hive 是基于 Hadoop 的一个数据仓库工具, 可以将结构化的数据文件映射为一张数据库表, 并提供简单的sql查询功能, 可以将sql语句转换为MapReduce任务进行运行</p>
<p>说明: Hive 是基于 Hadoop 安装的, 所以必须符合以下条件</p>
<ol>
<li>拥有已安装 Hadoop 的服务器集群</li>
<li>确保 Hadoop 集群正常运行 (启动 Hive 或 使用 Hadoop 命令时需要)</li>
<li>拥有已安装 Mysql 或 Mssql 的服务器 (本安装教程使用的数据库为 Mysql-5.6)</li>
</ol>
<h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><h3 id="0-配置要求"><a href="#0-配置要求" class="headerlink" title="0. 配置要求"></a>0. 配置要求</h3><h4 id="a-物理机"><a href="#a-物理机" class="headerlink" title="a) 物理机"></a>a) 物理机</h4><ul>
<li>处理器: 无要求</li>
<li>内存: 至少4G (建议8G或以上)</li>
<li>硬盘: 至少30G (建议50G或以上)</li>
<li>网卡: 可以上网即可</li>
</ul>
<h4 id="b-虚拟机"><a href="#b-虚拟机" class="headerlink" title="b) 虚拟机"></a>b) 虚拟机</h4><ul>
<li>数量: 至少3台 (1台Master、2台Worker)</li>
<li>处理器: 至少1个1核 (建议2个1核或以上)</li>
<li>内存: 至少512M (建议1G或以上)</li>
<li>硬盘: 至少10G (建议20G或以上)</li>
<li>网卡: 使用NAT模式即可</li>
</ul>
<h4 id="c-数据库"><a href="#c-数据库" class="headerlink" title="c) 数据库"></a>c) 数据库</h4><ul>
<li>数据库选择: Mysql 或 Mssql</li>
<li>权限要求: 至少拥有某个数据库的管理权</li>
</ul>
<p><strong>本安装教程使用的虚拟机配置如下</strong><br>Hadoop - Master:<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.100/24 - hadoop.master)<br>Hadoop - Worker1<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.110/24 - hadoop.worker1)<br>Hadoop - Worker2<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.111/24 - hadoop.worker2)<br>Mysql - root/123456<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.90/24)</p>
<h3 id="1-软件工具下载"><a href="#1-软件工具下载" class="headerlink" title="1. 软件工具下载"></a>1. 软件工具下载</h3><ul>
<li><a href="https://www-us.apache.org/dist/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz" target="_blank" rel="noopener">Apache Hive 3.1.1</a></li>
<li><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">Mysql-5.1.47</a> - (本安装教程的驱动包)</li>
<li><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/8.0.13/mysql-connector-java-8.0.13.jar" target="_blank" rel="noopener">Mysql-8.0.13</a></li>
<li><a href="http://central.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/7.1.3.jre11-preview/mssql-jdbc-7.1.3.jre11-preview.jar" target="_blank" rel="noopener">Mssql-7.1.3</a></li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=622" target="_blank" rel="noopener">Xshell 6</a> –&gt; 个人免费, License type 选择 Home and school use, 邮箱获取下载链接</li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=623" target="_blank" rel="noopener">Xftp 6</a> –&gt; 同上</li>
</ul>
<h3 id="2-基础环境搭建"><a href="#2-基础环境搭建" class="headerlink" title="2. 基础环境搭建"></a>2. 基础环境搭建</h3><h4 id="a-安装-Hadoop-的-CentOS-7-服务器集群"><a href="#a-安装-Hadoop-的-CentOS-7-服务器集群" class="headerlink" title="a) 安装 Hadoop 的 CentOS 7 服务器集群"></a>a) 安装 Hadoop 的 CentOS 7 服务器集群</h4><p>参考<a href="https://yongjie-xie.github.io/2018/10/10/HadoopInstall">这里</a></p>
<h4 id="b-安装-Docker-容器-CentOS-Ubuntu-lt-–-使用容器安装-Mysql-或-Mssql-方便高效"><a href="#b-安装-Docker-容器-CentOS-Ubuntu-lt-–-使用容器安装-Mysql-或-Mssql-方便高效" class="headerlink" title="b) 安装 Docker 容器 (CentOS / Ubuntu) &lt;– 使用容器安装 Mysql 或 Mssql 方便高效"></a>b) 安装 Docker 容器 (CentOS / Ubuntu) &lt;– 使用容器安装 Mysql 或 Mssql 方便高效</h4><p>参考<a href="https://yongjie-xie.github.io/2018/12/6/DockerInstall">这里</a></p>
<h4 id="c-安装-Xshell-6-和-Xftp-6-物理机"><a href="#c-安装-Xshell-6-和-Xftp-6-物理机" class="headerlink" title="c) 安装 Xshell 6 和 Xftp 6 (物理机)"></a>c) 安装 Xshell 6 和 Xftp 6 (物理机)</h4><p>过程略</p>
<h2 id="二、安装-Hive"><a href="#二、安装-Hive" class="headerlink" title="二、安装 Hive"></a>二、安装 Hive</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-Hive-安装文件下载和解压"><a href="#1-Hive-安装文件下载和解压" class="headerlink" title="1. Hive 安装文件下载和解压"></a>1. Hive 安装文件下载和解压</h3><p><strong><em>注意: Hive 放在 home 目录下, 也就是 /home/hadoop/ 目录</em></strong><br>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line">wget https://www-us.apache.org/dist/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz</span><br><span class="line">tar zxf apache-hive-3.1.1-bin.tar.gz</span><br><span class="line">mv apache-hive-3.1.1-bin hive</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# su hadoop</span><br><span class="line">[hadoop@hadoop root]$ cd</span><br><span class="line">[hadoop@hadoop ~]$ wget https://www-us.apache.org/dist/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz</span><br><span class="line">--2018-12-06 12:59:23--  https://www-us.apache.org/dist/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz</span><br><span class="line">Resolving www-us.apache.org (www-us.apache.org)... 40.79.78.1</span><br><span class="line">Connecting to www-us.apache.org (www-us.apache.org)|40.79.78.1|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 280944629 (268M) [application/x-gzip]</span><br><span class="line">Saving to: ‘apache-hive-3.1.1-bin.tar.gz’</span><br><span class="line"></span><br><span class="line">100%[==================================================================================&gt;] 280,944,629 11.0MB/s   in 29s    </span><br><span class="line"></span><br><span class="line">2018-12-06 12:59:59 (9.18 MB/s) - ‘apache-hive-3.1.1-bin.tar.gz’ saved [280944629/280944629]</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop ~]$ tar zxf apache-hive-3.1.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop ~]$ mv apache-hive-3.1.1-bin hive</span><br></pre></td></tr></table></figure></p>
<h3 id="2-驱动包部署"><a href="#2-驱动包部署" class="headerlink" title="2. 驱动包部署"></a>2. 驱动包部署</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/hive/lib/</span><br><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop conf]$ cd ~/hive/lib/</span><br><span class="line">[hadoop@hadoop lib]$ wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br><span class="line">--2018-12-06 15:45:45--  http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br><span class="line">Resolving central.maven.org (central.maven.org)... 151.101.24.209</span><br><span class="line">Connecting to central.maven.org (central.maven.org)|151.101.24.209|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1007502 (984K) [application/java-archive]</span><br><span class="line">Saving to: ‘mysql-connector-java-5.1.47.jar’</span><br><span class="line"></span><br><span class="line">100%[==================================================================================&gt;] 1,007,502    679KB/s   in 1.4s   </span><br><span class="line"></span><br><span class="line">2018-12-06 15:45:52 (679 KB/s) - ‘mysql-connector-java-5.1.47.jar’ saved [1007502/1007502]</span><br></pre></td></tr></table></figure></p>
<h2 id="三、配置-Hive"><a href="#三、配置-Hive" class="headerlink" title="三、配置 Hive"></a>三、配置 Hive</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-hive-site-xml"><a href="#1-hive-site-xml" class="headerlink" title="1. hive-site.xml"></a>1. hive-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cp ~/hive/conf/hive-default.xml.template ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#\$&#123;system:java.io.tmpdir&#125;#/home/hadoop/hive/tmp#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#\$&#123;system:user.name&#125;#hadoop#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s@&amp;#8;@ @g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#jdbc:derby:;databaseName=metastore_db;create=true#ConnectionURL#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#&lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver#&lt;value&gt;DriverName#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#&lt;value&gt;APP#&lt;value&gt;Username#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line">sed -i <span class="string">"s#&lt;value&gt;mine#&lt;value&gt;Password#g"</span> ~/hive/conf/hive-site.xml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请修改上述命令中的 [ConnectionURL, DriverName, Username, Password], 如下所示</span></span><br><span class="line"><span class="comment"># ConnectionURL:</span></span><br><span class="line"><span class="comment"># mysql - jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</span></span><br><span class="line"><span class="comment"># mssql - jdbc:sqlserver://localhost:1433;databaseName=hive;create=true</span></span><br><span class="line"><span class="comment"># DriverName:</span></span><br><span class="line"><span class="comment"># mysql - com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment"># mssql - com.microsoft.sqlserver.jdbc.SQLServerDriver</span></span><br><span class="line"><span class="comment"># Username, Password 替换为正确的用户名和密码</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop conf]$ cp ~/hive/conf/hive-default.xml.template ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#\$&#123;system:java.io.tmpdir&#125;#/home/hadoop/hive/tmp#g&quot; ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#\$&#123;system:user.name&#125;#hadoop#g&quot; ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#jdbc:derby:;databaseName=metastore_db;create=true#jdbc:mysql://192.168.80.90:3306/hive?createDatabaseIfNotExist=true#g&quot; ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#&lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver#&lt;value&gt;com.mysql.jdbc.Driver#g&quot; ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#&lt;value&gt;APP#&lt;value&gt;root#g&quot; ~/hive/conf/hive-site.xml</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s#&lt;value&gt;mine#&lt;value&gt;123456#g&quot; ~/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure></p>
<h3 id="2-hive-env-sh"><a href="#2-hive-env-sh" class="headerlink" title="2. hive-env.sh"></a>2. hive-env.sh</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp ~/hive/conf/hive-env.sh.template ~/hive/conf/hive-env.sh</span><br><span class="line">sed -i <span class="string">"s@# HADOOP_HOME=\$&#123;bin&#125;/../../hadoop@export HADOOP_HOME=<span class="variable">$HADOOP_HOME</span>@g"</span> ~/hive/conf/hive-env.sh</span><br><span class="line">sed -i <span class="string">"s@# export HIVE_CONF_DIR=@export HIVE_CONF_DIR=<span class="variable">$HIVE_CONF_DIR</span>@g"</span> ~/hive/conf/hive-env.sh</span><br><span class="line">sed -i <span class="string">"s@# export HIVE_AUX_JARS_PATH=@export HIVE_AUX_JARS_PATH=<span class="variable">$HIVE_HOME</span>/lib@g"</span> ~/hive/conf/hive-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前请确保环境变量设置正确</span></span><br><span class="line"><span class="comment"># $HADOOP_HOME $HIVE_HOME $HIVE_CONF_DIR</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop conf]$ cp ~/hive/conf/hive-env.sh.template ~/hive/conf/hive-env.sh</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s@# HADOOP_HOME=\$&#123;bin&#125;/../../hadoop@export HADOOP_HOME=$HADOOP_HOME@g&quot; ~/hive/conf/hive-env.sh</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s@# export HIVE_CONF_DIR=@export JAVA_HOME=$HIVE_CONF_DIR@g&quot; ~/hive/conf/hive-env.sh</span><br><span class="line">[hadoop@hadoop conf]$ sed -i &quot;s@# export HIVE_AUX_JARS_PATH=@export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib@g&quot; ~/hive/conf/hive-env.sh</span><br><span class="line">[hadoop@hadoop conf]$ cat  ~/hive/conf/hive-env.sh</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"># or more contributor license agreements.  See the NOTICE file</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># Set HADOOP_HOME to point to a specific hadoop install directory</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line"></span><br><span class="line"># Hive Configuration Directory can be controlled by:</span><br><span class="line">export HIVE_CONF_DIR=/home/hadoop/hive/conf</span><br><span class="line"></span><br><span class="line"># Folder containing extra libraries required for hive compilation/execution can be controlled by:</span><br><span class="line">export HIVE_AUX_JARS_PATH=/home/hadoop/hive/lib</span><br></pre></td></tr></table></figure></p>
<h3 id="3-环境变量-bashrc"><a href="#3-环境变量-bashrc" class="headerlink" title="3. 环境变量 .bashrc"></a>3. 环境变量 .bashrc</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">cat ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ vim ~/.bashrc </span><br><span class="line">[hadoop@hadoop ~]$ cat ~/.bashrc </span><br><span class="line"># .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line">if [ -f /etc/bashrc ]; then</span><br><span class="line">    . /etc/bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Uncomment the following line if you don&apos;t like systemctl&apos;s auto-paging feature:</span><br><span class="line"># export SYSTEMD_PAGER=</span><br><span class="line"></span><br><span class="line"># User specific aliases and functions</span><br><span class="line"></span><br><span class="line"># Hadoop Env</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HIVE_HOME=/home/hadoop/hive // 增加</span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf // 增加</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin // 修改</span><br><span class="line">[hadoop@hadoop ~]$ source ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<h3 id="4-创建必须目录-请确保-Hadoop-正常运行"><a href="#4-创建必须目录-请确保-Hadoop-正常运行" class="headerlink" title="4. 创建必须目录 (请确保 Hadoop 正常运行)"></a>4. 创建必须目录 (请确保 Hadoop 正常运行)</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line">hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">hadoop fs -chmod 777 /user/hive/warehouse</span><br><span class="line">hadoop fs -mkdir -p /tmp/hive</span><br><span class="line">hadoop fs -chmod 777 /tmp/hive</span><br><span class="line">hadoop fs -ls /user/hive/</span><br><span class="line">hadoop fs -ls /tmp/</span><br><span class="line">[hadoop@hadoop hive]$ mkdir -p ~/hive/tmp</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop hive]$ jps</span><br><span class="line">2036 ResourceManager</span><br><span class="line">2344 Jps</span><br><span class="line">1785 SecondaryNameNode</span><br><span class="line">1597 NameNode</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">2018-12-06 16:36:10,977 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -chmod 777 /user/hive/warehouse</span><br><span class="line">2018-12-06 16:36:38,038 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -mkdir -p /tmp/hive</span><br><span class="line">2018-12-06 16:36:46,047 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -chmod 777 /tmp/hive</span><br><span class="line">2018-12-06 16:36:50,335 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -ls /user/hive/</span><br><span class="line">2018-12-06 16:37:00,295 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxrwx   - hadoop supergroup          0 2018-12-06 16:36 /user/hive/warehouse</span><br><span class="line">[hadoop@hadoop hive]$ hadoop fs -ls /tmp/</span><br><span class="line">2018-12-06 16:37:06,519 WARN util.NativeCodeLoader: ... using builtin-java classes where applicable</span><br><span class="line">Found 3 items</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-11-22 16:05 /tmp/hadoop-yarn</span><br><span class="line">drwxrwxrwx   - hadoop supergroup          0 2018-12-06 16:36 /tmp/hive</span><br><span class="line">drwxrwxrwt   - hadoop wheel               0 2018-11-22 16:49 /tmp/logs</span><br><span class="line">[hadoop@hadoop hive]$ mkdir -p ~/hive/tmp</span><br></pre></td></tr></table></figure></p>
<h2 id="三、启动-Hive"><a href="#三、启动-Hive" class="headerlink" title="三、启动 Hive"></a>三、启动 Hive</h2><h3 id="1-初始化数据库"><a href="#1-初始化数据库" class="headerlink" title="1. 初始化数据库"></a>1. 初始化数据库</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop hive]$ schematool -initSchema -dbType mysql</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL:    jdbc:mysql://192.168.80.90:3306/hive?createDatabaseIfNotExist=true</span><br><span class="line">Metastore Connection Driver :    com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User:   root</span><br><span class="line">Starting metastore schema initialization to 3.1.0</span><br><span class="line">Initialization script hive-schema-3.1.0.mysql.sql</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br></pre></td></tr></table></figure></p>
<h3 id="2-启动并测试-Hive"><a href="#2-启动并测试-Hive" class="headerlink" title="2. 启动并测试 Hive"></a>2. 启动并测试 Hive</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive&gt; show <span class="built_in">functions</span>;</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop hive]$ hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">which: no hbase in (/usr/local/jdk1.8.0_181/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/home/hadoop/hive/bin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = 48da2fa7-57f6-4dcc-a2e7-81991fcceb94</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/hive/lib/hive-common-3.1.1.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = d0459c0a-d421-4062-a6ef-3a060e244791</span><br><span class="line">hive&gt; show functions;</span><br><span class="line">OK</span><br><span class="line">!</span><br><span class="line">!=</span><br><span class="line">$sum0</span><br><span class="line">%</span><br><span class="line">&amp;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">|</span><br><span class="line">~</span><br><span class="line">Time taken: 1.601 seconds, Fetched: 289 row(s)</span><br></pre></td></tr></table></figure></p>
<p><strong><em>Hive 安装结束</em></strong></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title='0' data-url='http://link.hhtjim.com/163/35032211.mp3'></li>
                    
                        <li title='1' data-url='http://link.hhtjim.com/163/41500546.mp3'></li>
                    
                        <li title='2' data-url='http://link.hhtjim.com/163/555557845.mp3'></li>
                    
                        <li title='3' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='4' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='5' data-url='http://link.hhtjim.com/163/409650440.mp3'></li>
                    
                        <li title='6' data-url='http://link.hhtjim.com/163/409654855.mp3'></li>
                    
                        <li title='7' data-url='http://link.hhtjim.com/163/407761545.mp3'></li>
                    
                        <li title='8' data-url='http://link.hhtjim.com/163/409654856.mp3'></li>
                    
                        <li title='9' data-url='http://link.hhtjim.com/163/466327445.mp3'></li>
                    
                        <li title='10' data-url='http://link.hhtjim.com/163/409654857.mp3'></li>
                    
                        <li title='11' data-url='http://link.hhtjim.com/163/405987145.mp3'></li>
                    
                        <li title='12' data-url='http://link.hhtjim.com/163/474945732.mp3'></li>
                    
                        <li title='13' data-url='http://link.hhtjim.com/163/409654858.mp3'></li>
                    
                        <li title='14' data-url='http://link.hhtjim.com/163/452578261.mp3'></li>
                    
                        <li title='15' data-url='http://link.hhtjim.com/163/409654859.mp3'></li>
                    
                        <li title='16' data-url='http://link.hhtjim.com/163/435307781.mp3'></li>
                    
                        <li title='17' data-url='http://link.hhtjim.com/163/477145631.mp3'></li>
                    
                        <li title='18' data-url='http://link.hhtjim.com/163/456175343.mp3'></li>
                    
                        <li title='19' data-url='http://link.hhtjim.com/163/409654860.mp3'></li>
                    
                        <li title='20' data-url='http://link.hhtjim.com/163/408250066.mp3'></li>
                    
                        <li title='21' data-url='http://link.hhtjim.com/163/537262197.mp3'></li>
                    
                        <li title='22' data-url='http://link.hhtjim.com/163/447978200.mp3'></li>
                    
                        <li title='23' data-url='http://link.hhtjim.com/163/439625573.mp3'></li>
                    
                        <li title='24' data-url='http://link.hhtjim.com/163/457840567.mp3'></li>
                    
                        <li title='25' data-url='http://link.hhtjim.com/163/452575951.mp3'></li>
                    
                        <li title='26' data-url='http://link.hhtjim.com/163/454966416.mp3'></li>
                    
                        <li title='27' data-url='http://link.hhtjim.com/163/419375942.mp3'></li>
                    
                        <li title='28' data-url='http://link.hhtjim.com/163/411988718.mp3'></li>
                    
                        <li title='29' data-url='http://link.hhtjim.com/163/418279656.mp3'></li>
                    
                        <li title='30' data-url='http://link.hhtjim.com/163/417594827.mp3'></li>
                    
                        <li title='31' data-url='http://link.hhtjim.com/163/423104116.mp3'></li>
                    
                        <li title='32' data-url='http://link.hhtjim.com/163/452577615.mp3'></li>
                    
                        <li title='33' data-url='http://link.hhtjim.com/163/436675327.mp3'></li>
                    
                        <li title='34' data-url='http://link.hhtjim.com/163/413074476.mp3'></li>
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
        data-ae='false'
        data-ci='6f2ced5993cec2abfdc0'
        data-cs='53941abd6c61cb31c982bddf0394826699a4b9a7'
        data-r='YongJie-Xie.github.io'
        data-o='YongJie-Xie'
        data-a='YongJie-Xie'
        data-d='false'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>