
<!DOCTYPE html>
<html lang="zh-cn" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>抱着西瓜砸地球的博客</title>

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Blog,"> 
    <meta name="description" content="OpenStack 是一个云操作系统，通过数据中心可控制大型的计算、存储、网络等资源池。所有的管理通过前端界面管理员就可以完成，同样也可以通过 web 接口让最终用户部署资源。
参考文档: 

官方,"> 
    <meta name="author" content="YongJie-Xie"> 
    <link rel="alternative" href="atom.xml" title="抱着西瓜砸地球的博客" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
</head>

<body class="loading">
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">Sqoop 安装</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>
    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Sqoop 安装</h1>
        <div class="stuff">
            <span>十二月 12, 2018</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/CentOS/">CentOS</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hive/">Hive</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Java/">Java</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Sqoop/">Sqoop</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/VMware/">VMware</a></li></ul>


        </div>
        <div class="content markdown">
            <p>Sqoop 是 HDFS、Hive、HBase 和 RDBMS 结构化数据库之间传输大量数据的工具</p>
<p>说明: Sqoop 是基于 Hadoop 安装的, 所以必须符合以下条件</p>
<ol>
<li>拥有已安装 Hadoop 的服务器集群</li>
<li>确保 Hadoop 集群正常运行 (启动 Hive 或 使用 Hadoop 命令时需要)</li>
<li>拥有已安装 Mysql 或 Mssql 的服务器 (本安装教程使用的数据库为 Mysql-5.6)</li>
</ol>
<h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><h3 id="0-配置要求"><a href="#0-配置要求" class="headerlink" title="0. 配置要求"></a>0. 配置要求</h3><h4 id="a-物理机"><a href="#a-物理机" class="headerlink" title="a) 物理机"></a>a) 物理机</h4><ul>
<li>处理器: 无要求</li>
<li>内存: 至少4G (建议8G或以上)</li>
<li>硬盘: 至少30G (建议50G或以上)</li>
<li>网卡: 可以上网即可</li>
</ul>
<h4 id="b-虚拟机"><a href="#b-虚拟机" class="headerlink" title="b) 虚拟机"></a>b) 虚拟机</h4><ul>
<li>数量: 至少3台 (1台Master、2台Worker)</li>
<li>处理器: 至少1个1核 (建议2个1核或以上)</li>
<li>内存: 至少512M (建议1G或以上)</li>
<li>硬盘: 至少10G (建议20G或以上)</li>
<li>网卡: 使用NAT模式即可</li>
</ul>
<h4 id="c-数据库"><a href="#c-数据库" class="headerlink" title="c) 数据库"></a>c) 数据库</h4><ul>
<li>数据库选择: Mysql 或 Mssql</li>
<li>权限要求: 至少拥有某个数据库的管理权</li>
</ul>
<p><strong>本安装教程使用的虚拟机配置如下</strong><br>Hadoop - Master:<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.100/24 - hadoop.master)<br>Hadoop - Worker1<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.110/24 - hadoop.worker1)<br>Hadoop - Worker2<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.111/24 - hadoop.worker2)<br>Mysql - root/123456<br>内存(1G), 处理器(单核), 硬盘(20G), 网络(NAT - 192.168.80.90/24)</p>
<h3 id="1-软件工具下载"><a href="#1-软件工具下载" class="headerlink" title="1. 软件工具下载"></a>1. 软件工具下载</h3><ul>
<li><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz" target="_blank" rel="noopener">Apache Sqoop 1.4.7</a></li>
<li><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">Mysql-5.1.47</a> - (本安装教程的驱动包)</li>
<li><a href="http://central.maven.org/maven2/mysql/mysql-connector-java/8.0.13/mysql-connector-java-8.0.13.jar" target="_blank" rel="noopener">Mysql-8.0.13</a></li>
<li><a href="http://central.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/7.1.3.jre11-preview/mssql-jdbc-7.1.3.jre11-preview.jar" target="_blank" rel="noopener">Mssql-7.1.3</a></li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=622" target="_blank" rel="noopener">Xshell 6</a> –&gt; 个人免费, License type 选择 Home and school use, 邮箱获取下载链接</li>
<li><a href="http://www.netsarang.com/download/down_form.html?code=623" target="_blank" rel="noopener">Xftp 6</a> –&gt; 同上</li>
</ul>
<h3 id="2-基础环境搭建"><a href="#2-基础环境搭建" class="headerlink" title="2. 基础环境搭建"></a>2. 基础环境搭建</h3><h4 id="a-安装-Hadoop-的-CentOS-7-服务器集群"><a href="#a-安装-Hadoop-的-CentOS-7-服务器集群" class="headerlink" title="a) 安装 Hadoop 的 CentOS 7 服务器集群"></a>a) 安装 Hadoop 的 CentOS 7 服务器集群</h4><p>参考<a href="https://yongjie-xie.github.io/2018/10/10/HadoopInstall">这里</a></p>
<h4 id="b-安装-Docker-容器-CentOS-Ubuntu-lt-–-使用容器安装-Mysql-或-Mssql-方便高效"><a href="#b-安装-Docker-容器-CentOS-Ubuntu-lt-–-使用容器安装-Mysql-或-Mssql-方便高效" class="headerlink" title="b) 安装 Docker 容器 (CentOS / Ubuntu) &lt;– 使用容器安装 Mysql 或 Mssql 方便高效"></a>b) 安装 Docker 容器 (CentOS / Ubuntu) &lt;– 使用容器安装 Mysql 或 Mssql 方便高效</h4><p>参考<a href="https://yongjie-xie.github.io/2018/12/6/DockerInstall">这里</a></p>
<h4 id="c-安装-Xshell-6-和-Xftp-6-物理机"><a href="#c-安装-Xshell-6-和-Xftp-6-物理机" class="headerlink" title="c) 安装 Xshell 6 和 Xftp 6 (物理机)"></a>c) 安装 Xshell 6 和 Xftp 6 (物理机)</h4><p>过程略</p>
<h2 id="二、安装-Sqoop"><a href="#二、安装-Sqoop" class="headerlink" title="二、安装 Sqoop"></a>二、安装 Sqoop</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-Hive-安装文件下载和解压"><a href="#1-Hive-安装文件下载和解压" class="headerlink" title="1. Hive 安装文件下载和解压"></a>1. Hive 安装文件下载和解压</h3><p><strong><em>注意: Hive 放在 home 目录下, 也就是 /home/hadoop/ 目录</em></strong><br>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop ~]# su hadoop</span><br><span class="line">[hadoop@hadoop root]$ cd</span><br><span class="line">[hadoop@hadoop ~]$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">--2018-12-12 13:58:04--  https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.8.193, 2402:f000:1:408:8100::1</span><br><span class="line">Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.8.193|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 17953604 (17M) [application/x-gzip]</span><br><span class="line">Saving to: ‘sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz’</span><br><span class="line"></span><br><span class="line">100%[==================================================================================&gt;] 17,953,604  1.16MB/s   in 14s    </span><br><span class="line"></span><br><span class="line">2018-12-12 13:58:17 (1.26 MB/s) - ‘sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz’ saved [17953604/17953604]</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop ~]$ tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">[hadoop@hadoop ~]$ mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop</span><br></pre></td></tr></table></figure></p>
<h3 id="2-驱动包部署"><a href="#2-驱动包部署" class="headerlink" title="2. 驱动包部署"></a>2. 驱动包部署</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/sqoop/lib/</span><br><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ cd ~/sqoop/lib/</span><br><span class="line">[hadoop@hadoop lib]$ wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br><span class="line">--2018-12-12 14:35:20--  http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br><span class="line">Resolving central.maven.org (central.maven.org)... 151.101.40.209</span><br><span class="line">Connecting to central.maven.org (central.maven.org)|151.101.40.209|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1007502 (984K) [application/java-archive]</span><br><span class="line">Saving to: ‘mysql-connector-java-5.1.47.jar’</span><br><span class="line"></span><br><span class="line">100%[==================================================================================&gt;] 1,007,502   1.05MB/s   in 0.9s   </span><br><span class="line"></span><br><span class="line">2018-12-12 14:35:21 (1.05 MB/s) - ‘mysql-connector-java-5.1.47.jar’ saved [1007502/1007502]</span><br></pre></td></tr></table></figure></p>
<h2 id="三、配置-Sqoop"><a href="#三、配置-Sqoop" class="headerlink" title="三、配置 Sqoop"></a>三、配置 Sqoop</h2><p><strong><em>注意: 必须在 Master 的 hadoop 用户下执行, 无需在 Worker1、Worker2 服务器中执行</em></strong></p>
<h3 id="1-sqoop-site-xml"><a href="#1-sqoop-site-xml" class="headerlink" title="1. sqoop-site.xml"></a>1. sqoop-site.xml</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp ~/sqoop/conf/sqoop-site-template.xml ~/sqoop/conf/sqoop-site.xml</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ cp ~/sqoop/conf/sqoop-site-template.xml ~/sqoop/conf/sqoop-site.xml</span><br></pre></td></tr></table></figure></p>
<h3 id="2-sqoop-env-sh"><a href="#2-sqoop-env-sh" class="headerlink" title="2. sqoop-env.sh"></a>2. sqoop-env.sh</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp ~/sqoop/conf/sqoop-env-template.sh ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">sed -i <span class="string">"s@#export HADOOP_COMMON_HOME=@export HADOOP_COMMON_HOME=<span class="variable">$HADOOP_HOME</span>@g"</span> ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">sed -i <span class="string">"s@#export HADOOP_MAPRED_HOME=@export HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span>@g"</span> ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">sed -i <span class="string">"s@#export HIVE_HOME=@export HIVE_HOME=<span class="variable">$HIVE_HOME</span>@g"</span> ~/sqoop/conf/sqoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行前请确保环境变量设置正确, 设置环境变量请参考第3点</span></span><br><span class="line"><span class="comment"># $HADOOP_HOME $HIVE_HOME $HIVE_CONF_DIR</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ cp ~/sqoop/conf/sqoop-env-template.sh ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">[hadoop@hadoop ~]$ sed -i &quot;s@#export HADOOP_COMMON_HOME=@export HADOOP_COMMON_HOME=$HADOOP_HOME@g&quot; ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">[hadoop@hadoop ~]$ sed -i &quot;s@#export HADOOP_MAPRED_HOME=@export HADOOP_MAPRED_HOME=$HADOOP_HOME@g&quot; ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">[hadoop@hadoop ~]$ sed -i &quot;s@#export HIVE_HOME=@export HIVE_HOME=$HIVE_HOME@g&quot; ~/sqoop/conf/sqoop-env.sh</span><br><span class="line">[hadoop@hadoop ~]$ cat ~/sqoop/conf/sqoop-env.sh</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/hadoop</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/hadoop</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/home/hadoop/hive</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br></pre></td></tr></table></figure></p>
<h3 id="3-环境变量-bashrc"><a href="#3-环境变量-bashrc" class="headerlink" title="3. 环境变量 .bashrc"></a>3. 环境变量 .bashrc</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">cat ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop bin]$ vim ~/.bashrc </span><br><span class="line">[hadoop@hadoop bin]$ cat ~/.bashrc </span><br><span class="line"># .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line">if [ -f /etc/bashrc ]; then</span><br><span class="line">    . /etc/bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Uncomment the following line if you don&apos;t like systemctl&apos;s auto-paging feature:</span><br><span class="line"># export SYSTEMD_PAGER=</span><br><span class="line"></span><br><span class="line"># User specific aliases and functions</span><br><span class="line"></span><br><span class="line"># Hadoop Env</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HIVE_HOME=/home/hadoop/hive</span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf</span><br><span class="line">export SQOOP_HOME=/home/hadoop/sqoop // 增加</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin // 修改</span><br><span class="line">[hadoop@hadoop bin]$ source ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<h2 id="三、测试-Sqoop"><a href="#三、测试-Sqoop" class="headerlink" title="三、测试 Sqoop"></a>三、测试 Sqoop</h2><h3 id="1-列出数据库"><a href="#1-列出数据库" class="headerlink" title="1. 列出数据库"></a>1. 列出数据库</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect <span class="string">"ConnectionURL"</span> --username Username --password Password</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请修改上述命令中的 [ConnectionURL, Username, Password], 如下所示</span></span><br><span class="line"><span class="comment"># ConnectionURL:</span></span><br><span class="line"><span class="comment"># mysql - jdbc:mysql://localhost:3306</span></span><br><span class="line"><span class="comment"># mssql - jdbc:sqlserver://localhost:1433</span></span><br><span class="line"><span class="comment"># Username, Password 替换为正确的用户名和密码</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop bin]$ sqoop list-databases --connect &quot;jdbc:mysql://192.168.80.90:3306&quot; --username root --password 123456</span><br><span class="line">Warning: /home/hadoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2018-12-12 15:33:23,689 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2018-12-12 15:33:23,962 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2018-12-12 15:33:24,748 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">information_schema</span><br><span class="line">hive</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br></pre></td></tr></table></figure></p>
<h3 id="2-查询导入数据"><a href="#2-查询导入数据" class="headerlink" title="2. 查询导入数据"></a>2. 查询导入数据</h3><p>命令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --driver <span class="string">"DriverName"</span> --connect <span class="string">"ConnectionURL"</span> --username Username --password Password --query <span class="string">"Sql"</span> --target-dir <span class="string">"HdfsDir"</span> --num-mappers 1 --fields-terminated-by <span class="string">"\t"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请修改上述命令中的 [DriverName, ConnectionURL, Username, Password, Sql, HdfsDir], 如下所示</span></span><br><span class="line"><span class="comment"># DriverName:</span></span><br><span class="line"><span class="comment"># mysql - com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment"># mssql - com.microsoft.sqlserver.jdbc.SQLServerDriver</span></span><br><span class="line"><span class="comment"># ConnectionURL:</span></span><br><span class="line"><span class="comment"># mysql - jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</span></span><br><span class="line"><span class="comment"># mssql - jdbc:sqlserver://localhost:1433;databaseName=hive;create=true</span></span><br><span class="line"><span class="comment"># Username, Password 替换为正确的用户名和密码</span></span><br><span class="line"><span class="comment"># Sql: 查询 SQL 语句</span></span><br><span class="line"><span class="comment"># HdfsDir: 查询结果存放目录, 如不存在, 请先创建</span></span><br><span class="line"><span class="comment"># sqoop import --driver "com.mysql.jdbc.Driver" --connect "jdbc:mysql://192.168.80.90:3306/mysql" --username root --password 123456 --query "SELECT `user`,`password`,`host` FROM user;^C--target-dir "/user/mysql/test" --num-mappers 1 --fields-terminated-by "\t"</span></span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop bin]$ hadoop fs -mkdir /user/mysql</span><br><span class="line">2018-12-12 15:50:16,786 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop bin]$ sqoop import --driver &quot;com.mysql.jdbc.Driver&quot; --connect &quot;jdbc:mysql://192.168.80.90:3306/mysql&quot; --username root --password 123456 --query &quot;SELECT user,password,host FROM user WHERE \$CONDITIONS&quot; --target-dir &quot;/user/mysql/test&quot; --num-mappers 1 --fields-terminated-by &quot;\t&quot;</span><br><span class="line">Warning: /home/hadoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2018-12-12 15:54:04,073 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2018-12-12 15:54:04,220 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2018-12-12 15:54:04,436 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.</span><br><span class="line">2018-12-12 15:54:04,535 INFO manager.SqlManager: Using default fetchSize of 1000</span><br><span class="line">2018-12-12 15:54:04,535 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">2018-12-12 15:54:05,648 INFO manager.SqlManager: Executing SQL statement: SELECT user,password,host FROM user WHERE  (1 = 0) </span><br><span class="line">2018-12-12 15:54:05,665 INFO manager.SqlManager: Executing SQL statement: SELECT user,password,host FROM user WHERE  (1 = 0) </span><br><span class="line">2018-12-12 15:54:05,740 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/hadoop</span><br><span class="line">Note: /tmp/sqoop-hadoop/compile/e00cfeb4145907c9514185fe00ef4b92/QueryResult.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation for details.</span><br><span class="line">2018-12-12 15:54:16,274 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/e00cfeb4145907c9514185fe00ef4b92/QueryResult.jar</span><br><span class="line">2018-12-12 15:54:16,353 INFO mapreduce.ImportJobBase: Beginning query import.</span><br><span class="line">2018-12-12 15:54:16,355 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address</span><br><span class="line">2018-12-12 15:54:16,830 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2018-12-12 15:54:17,011 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">2018-12-12 15:54:21,809 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">2018-12-12 15:54:22,326 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.80.100:8032</span><br><span class="line">2018-12-12 15:54:26,240 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1544600654373_0001</span><br><span class="line">2018-12-12 15:54:38,640 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">2018-12-12 15:54:38,873 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">2018-12-12 15:54:39,021 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">2018-12-12 15:54:39,609 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544600654373_0001</span><br><span class="line">2018-12-12 15:54:39,629 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2018-12-12 15:54:40,845 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2018-12-12 15:54:40,848 INFO resource.ResourceUtils: Unable to find &apos;resource-types.xml&apos;.</span><br><span class="line">2018-12-12 15:54:41,831 INFO impl.YarnClientImpl: Submitted application application_1544600654373_0001</span><br><span class="line">2018-12-12 15:54:41,985 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1544600654373_0001/</span><br><span class="line">2018-12-12 15:54:41,988 INFO mapreduce.Job: Running job: job_1544600654373_0001</span><br><span class="line">2018-12-12 15:55:33,251 INFO mapreduce.Job: Job job_1544600654373_0001 running in uber mode : false</span><br><span class="line">2018-12-12 15:55:33,253 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2018-12-12 15:56:11,922 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2018-12-12 15:56:12,998 INFO mapreduce.Job: Job job_1544600654373_0001 completed successfully</span><br><span class="line">2018-12-12 15:56:13,516 INFO mapreduce.Job: Counters: 32</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=0</span><br><span class="line">        FILE: Number of bytes written=222520</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=87</span><br><span class="line">        HDFS: Number of bytes written=155</span><br><span class="line">        HDFS: Number of read operations=6</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Other local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=58918</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">        Total time spent by all map tasks (ms)=29459</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=29459</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=30166016</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=3</span><br><span class="line">        Map output records=3</span><br><span class="line">        Input split bytes=87</span><br><span class="line">        Spilled Records=0</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=0</span><br><span class="line">        GC time elapsed (ms)=505</span><br><span class="line">        CPU time spent (ms)=5390</span><br><span class="line">        Physical memory (bytes) snapshot=132419584</span><br><span class="line">        Virtual memory (bytes) snapshot=2740277248</span><br><span class="line">        Total committed heap usage (bytes)=22073344</span><br><span class="line">        Peak Map Physical memory (bytes)=132419584</span><br><span class="line">        Peak Map Virtual memory (bytes)=2740277248</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=0</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=155</span><br><span class="line">2018-12-12 15:56:13,568 INFO mapreduce.ImportJobBase: Transferred 155 bytes in 99.7636 seconds (1.5537 bytes/sec)</span><br><span class="line">2018-12-12 15:56:13,583 INFO mapreduce.ImportJobBase: Retrieved 3 records.</span><br><span class="line">[hadoop@hadoop bin]$ hadoop fs -ls /user/mysql/test/</span><br><span class="line">2018-12-12 15:56:39,788 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup          0 2018-12-12 15:56 /user/mysql/test/_SUCCESS</span><br><span class="line">-rw-r--r--   2 hadoop supergroup        155 2018-12-12 15:56 /user/mysql/test/part-m-00000</span><br><span class="line">[hadoop@hadoop bin]$ hadoop fs -cat /user/mysql/test/part-m-00000</span><br><span class="line">2018-12-12 15:57:01,962 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">root    *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9   localhost</span><br><span class="line">test    *94BDCEBE19083CE2A1F959FD02F964C7AF4CFC29   %</span><br><span class="line">root    *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9   %</span><br></pre></td></tr></table></figure></p>
<p><strong><em>Sqoop 安装结束</em></strong></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title='0' data-url='http://link.hhtjim.com/163/35032211.mp3'></li>
                    
                        <li title='1' data-url='http://link.hhtjim.com/163/41500546.mp3'></li>
                    
                        <li title='2' data-url='http://link.hhtjim.com/163/555557845.mp3'></li>
                    
                        <li title='3' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='4' data-url='http://link.hhtjim.com/163/493041272.mp3'></li>
                    
                        <li title='5' data-url='http://link.hhtjim.com/163/409650440.mp3'></li>
                    
                        <li title='6' data-url='http://link.hhtjim.com/163/409654855.mp3'></li>
                    
                        <li title='7' data-url='http://link.hhtjim.com/163/407761545.mp3'></li>
                    
                        <li title='8' data-url='http://link.hhtjim.com/163/409654856.mp3'></li>
                    
                        <li title='9' data-url='http://link.hhtjim.com/163/466327445.mp3'></li>
                    
                        <li title='10' data-url='http://link.hhtjim.com/163/409654857.mp3'></li>
                    
                        <li title='11' data-url='http://link.hhtjim.com/163/405987145.mp3'></li>
                    
                        <li title='12' data-url='http://link.hhtjim.com/163/474945732.mp3'></li>
                    
                        <li title='13' data-url='http://link.hhtjim.com/163/409654858.mp3'></li>
                    
                        <li title='14' data-url='http://link.hhtjim.com/163/452578261.mp3'></li>
                    
                        <li title='15' data-url='http://link.hhtjim.com/163/409654859.mp3'></li>
                    
                        <li title='16' data-url='http://link.hhtjim.com/163/435307781.mp3'></li>
                    
                        <li title='17' data-url='http://link.hhtjim.com/163/477145631.mp3'></li>
                    
                        <li title='18' data-url='http://link.hhtjim.com/163/456175343.mp3'></li>
                    
                        <li title='19' data-url='http://link.hhtjim.com/163/409654860.mp3'></li>
                    
                        <li title='20' data-url='http://link.hhtjim.com/163/408250066.mp3'></li>
                    
                        <li title='21' data-url='http://link.hhtjim.com/163/537262197.mp3'></li>
                    
                        <li title='22' data-url='http://link.hhtjim.com/163/447978200.mp3'></li>
                    
                        <li title='23' data-url='http://link.hhtjim.com/163/439625573.mp3'></li>
                    
                        <li title='24' data-url='http://link.hhtjim.com/163/457840567.mp3'></li>
                    
                        <li title='25' data-url='http://link.hhtjim.com/163/452575951.mp3'></li>
                    
                        <li title='26' data-url='http://link.hhtjim.com/163/454966416.mp3'></li>
                    
                        <li title='27' data-url='http://link.hhtjim.com/163/419375942.mp3'></li>
                    
                        <li title='28' data-url='http://link.hhtjim.com/163/411988718.mp3'></li>
                    
                        <li title='29' data-url='http://link.hhtjim.com/163/418279656.mp3'></li>
                    
                        <li title='30' data-url='http://link.hhtjim.com/163/417594827.mp3'></li>
                    
                        <li title='31' data-url='http://link.hhtjim.com/163/423104116.mp3'></li>
                    
                        <li title='32' data-url='http://link.hhtjim.com/163/452577615.mp3'></li>
                    
                        <li title='33' data-url='http://link.hhtjim.com/163/436675327.mp3'></li>
                    
                        <li title='34' data-url='http://link.hhtjim.com/163/413074476.mp3'></li>
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
        data-ae='false'
        data-ci='6f2ced5993cec2abfdc0'
        data-cs='53941abd6c61cb31c982bddf0394826699a4b9a7'
        data-r='YongJie-Xie.github.io'
        data-o='YongJie-Xie'
        data-a='YongJie-Xie'
        data-d='false'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>